{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnyWtp/oWF/f3PN1T7qJ4s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arauber-psychologistcoder/ari-psycoder/blob/main/rede_neural_zero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ],
      "metadata": {
        "id": "z07HTqdnc2Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() #definindo a conversão de imagem para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) #carrega a parte de treino de dataset\n",
        "trainloader = torch.utils.data.DataLoader (trainset, batch_size=64, shuffle=True) #cria um buffer para pegar os dados por partes\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform) #carrega a parte de validação do dataset\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) #cria um buffer para pegar os dados por partes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aijUtgxleRJD",
        "outputId": "57a934bb-d6d1-4115-d0d1-d0df87bb349b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 46.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.11MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.69MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.71MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = next(dataiter)\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "tis9V56-gspM",
        "outputId": "85042fb0-da94-4578-d9eb-2c01e927832c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7974feab7e60>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape) #para verificar as dimensões do tensor de cada imagem\n",
        "print(etiquetas[0].shape) #para verificar as dimensões do tensor de cada etiqueta\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcRjfMMQrahU",
        "outputId": "3488cc6e-4f75-4b86-b8e5-8c741fe4e4d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "  def _init_(self):\n",
        "    super(Modelo, self) ._init_()\n",
        "    self.linear1 = nn.Linear(28*28, 128) #camada de entrada, 784 neurônios que se ligam a 128\n",
        "    self.linear2 = nn.Linear(128, 64) #camada interna 1, 128 neurônios que se ligam a 64\n",
        "    self.linear3 = nn.linear(64, 10) #camada interna 2,64 neurônios que se ligam a 10\n",
        "    #para cada camada de saída não é necessário definir nada pois só precisamos pegar o output da camada interna 2\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = F.relu(self.linear1(X)) #função de ativação da camadade entrada para a camada interna 1\n",
        "    X = F.relu(self.linear2(X)) #função de ativação da camada interna 1 para a camada interna 2\n",
        "    X = self.linear3(X) #função de ativaçãoda camada interna 2 para a camada de saída, nesse caso f(x) = x\n",
        "    return F.log_softmax (X, dim=1) #dados utilizados para calcular a perda"
      ],
      "metadata": {
        "id": "dadxsSIbuYY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "def treino(modelo, trainloader, device):\n",
        "\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) #define a política de atualizaçãodo pesos e bias\n",
        "  inicio = time() #timer para sabermos quanto tempo levou o treinamento\n",
        "\n",
        "  criterio = nn.NLLLoss() #definindo o critério para calcular a perda\n",
        "  EPOCHS = 10 #numero de epochs que o algoritimo rodará, o ideal é ao menos 100\n",
        "  modelo.train() #ativando o modo de treinamento do modelo\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0 #inicialização da perda acumulada em questão\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "\n",
        "      imagens = imagens.view(imagens.shape[0], -1).to(device) #convertendo as imagens para \"vetores\" de 28*28 casas para..\n",
        "      etiquetas = etiquetas.to(device)\n",
        "\n",
        "      otimizador.zero_grad() #zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "      output = modelo(imagens) #colocando os dados do modelo\n",
        "      perda_instantanea = criterio(output, etiquetas) #calculando a perda da epoch em questão\n",
        "\n",
        "      perda_instantanea.backward() #back propagation a partir da perda\n",
        "\n",
        "      otimizador.step() #atualizando os pesos e bias\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item() #atualização de perda acumulada\n",
        "\n",
        "    else:\n",
        "      print('Epoch {} - Perda resultante: {}'.format(\n",
        "          epoch+1, perda_acumulada/len(trainloader))\n",
        "      )\n",
        "\n",
        "  print('\\nTempo de treino (em minutos) =', (time()- inicio)/60)"
      ],
      "metadata": {
        "id": "50z7AsP9zGXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "    conta_corretas, conta_todas = 0, 0\n",
        "\n",
        "    # desativar o autograd para acelerar a validação, Grafos computacionais dinâmicos tem custo alto de processamento\n",
        "    with torch.no_grad():\n",
        "        for imagens, etiquetas in valloader:\n",
        "            for i in range(len(etiquetas)):\n",
        "                img = imagens[i].view(1, 784)\n",
        "\n",
        "                logps = modelo(img.to(device))  # output do modelo em escala algotitmica\n",
        "\n",
        "                ps = torch.exp(logps)  # converte output para escala normal (lembrando que é um tensor)\n",
        "                probab = list(ps.cpu().numpy()[0])\n",
        "\n",
        "                etiqueta_pred = probab.index(max(probab))  # converte o tensor em um número, no caso, o número é o modelo prévio\n",
        "                etiqueta_certa = etiquetas[i].item()       # valor correto da etiqueta\n",
        "\n",
        "                if etiqueta_certa == etiqueta_pred:  # compara a previsão com valor correto\n",
        "                    conta_corretas += 1\n",
        "\n",
        "                conta_todas += 1\n",
        "\n",
        "    print(\"Total de imagens testadas =\", conta_todas)\n",
        "    print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas * 100 / conta_todas))"
      ],
      "metadata": {
        "id": "UtezmUCiXnj_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Modelo(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(784, 128)\n",
        "        self.linear2 = nn.Linear(128, 64)\n",
        "        self.linear3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.log_softmax(self.linear3(x), dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lphg7aE2Z2f1"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}